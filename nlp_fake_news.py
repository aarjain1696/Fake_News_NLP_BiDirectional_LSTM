# -*- coding: utf-8 -*-
"""NLP_Fake News.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Lth_mikjCdG5K46LCPnVW049ijvLDlCB

## **Fake News Classifier using Bi-drectional LSTM**

Mount drive to read kaggle files from drive
"""

!pip install tensorflow-gpu==2.0

import pandas as pd
import numpy as np

import tensorflow as tf
tf.__version__     # have keras imbedded into tensorflow
from tensorflow.keras.layers import Embedding 
from tensorflow.keras.preprocessing.sequence import pad_sequences # to make sentenses of equal size
from tensorflow.keras.models import Sequential
from tensorflow.keras.preprocessing.text import one_hot
from tensorflow.keras.layers import LSTM
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Bidirectional  # to make LSTM RNN bidirectional 
from tensorflow.keras.layers import Dropout

import nltk
import re
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer

from sklearn.model_selection import train_test_split

from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report

df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/FakeNews_NLP_BiLSTM/fake-news/train.csv')

df.head(3)

df.shape

"""# Cleaing the dataset"""

# dropping Nan values
df = df.dropna()

df.shape

# sperating dependent feature (label column) and independent features 
x = df.drop('label', axis=1)
y = df['label']

# checking weather the dataset is balanced or not
y.value_counts()

x.shape

### Vocabulary size to specify dictionary size
voc_size = 5000

sms = x.copy()

sms['title'][1]

sms.reset_index(inplace=True)

# using nltk
nltk.download('stopwords')

"""**Data processing**


*   Removing specail charaters 
*   Lower case the text/message/news


*   Spliting by removing the stop words
*   Steming of the news
"""

## data preprocessing 
ps = PorterStemmer()
corpus = []
for i in range(0 , len(sms)):
  print(i)
  review = re.sub('[^a-zA-Z]', ' ', sms['title'][i]) # removing all special charaters except a-z and A-Z
  review = review.lower() #lower casing the text
  review = review.split() # split the sentense into individual words 

  review = [ps.stem(word) for word in review if not word in stopwords.words('english')] # removing stop words
  review = ' '.join(review) # joining back the sentense after removing special charaters, stopswords etc
  corpus.append(review) # putting back the processed sentense into review

# view all the words after processing 
corpus

"""# Assigning index to all words
Using one_hot to assign index based on the vocabulary size
"""

onehot_repr = [one_hot(words, voc_size) for words in corpus]
onehot_repr

"""# **Embedding Representation**
making all sentenses of same length
"""

sms_length = 20
embedded_docs = pad_sequences(onehot_repr, padding = 'pre', maxlen = sms_length)
print(embedded_docs)

embedded_docs[0]

"""# **Model Creation** 
LSTM model
"""

embedding_vector_feature = 40  #coverting embedded doc into a vector of 40, this will now take 40 features
model = Sequential()
model.add(Embedding(voc_size, embedding_vector_feature, input_length=sms_length))
model.add(Bidirectional(LSTM(100)))  # 2 lstm is build 
model.add(Dropout(0.3))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])
print(model.summary())

len(embedded_docs), y.shape

x_final = np.array(embedded_docs)
y_final = np.array(y)

"""# Spliting the data into test and train part"""

X_train, X_test, Y_train, Y_test = train_test_split(x_final, y_final, test_size = 0.35, random_state = 42)

"""# **Training the model**"""

model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=20, batch_size=64)

"""# **Model Performace**"""

y_pred = model.predict_classes(X_test)

confusion_matrix(Y_test, y_pred)

print(classification_report(Y_test, y_pred))

"""# **Result**
## **Accuracy: 91%**
"""